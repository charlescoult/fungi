{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518a0c58",
   "metadata": {},
   "source": [
    "# Model Generation for GBIF Fungi Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c086d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_version = \"0.0.1\"\n",
    "runs_dir = '/media/data/runs'\n",
    "runs_hdf = 'runs.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b411778a",
   "metadata": {},
   "source": [
    "## References\n",
    "* [Transfer Learning with Hub](https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub)\n",
    "* [`tf.data`: Build TensorFlow input pipelines](https://www.tensorflow.org/guide/data?hl=en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62b4d31",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a285c8a1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2c0f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from keras.utils.layer_utils import count_params\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a639289",
   "metadata": {},
   "source": [
    "### Limit GPU memory allocation\n",
    "[Limiting GPU Memory Growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16845159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_memory_growth(limit=True):\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, limit)\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10a47939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "limit_memory_growth()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106e1972",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ebd47",
   "metadata": {},
   "source": [
    "## Run Identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f625078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: generate run_id from hdf runs.hdf in runs_dir\n",
    "run_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bde63a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/data/runs/0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This run's path\n",
    "run_path = os.path.join( runs_dir, str(run_id) )\n",
    "run_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51f16734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run path already exists!!\n",
      " Overwriting: /media/data/runs/0\n"
     ]
    }
   ],
   "source": [
    "if (os.path.exists( run_path )):\n",
    "    print(\"Run path already exists!!\")\n",
    "    print(\" Overwriting: %s\" % run_path)\n",
    "else:\n",
    "    os.makedirs( run_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "280b9512",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94b709fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_dropout = 0.33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1833a622",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7175f0db",
   "metadata": {},
   "source": [
    "## Define Model Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a95bd61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of tuples describing the models to be tested\n",
    "# in the form: (model_handle, input_image_size, preprocessing_function)\n",
    "# where the model_handle is a model building function or a url to a tfhub feature model\n",
    "base_models = [\n",
    "    {\n",
    "        'source': 'https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4',\n",
    "        'input_dim': 224,\n",
    "        # https://www.tensorflow.org/hub/common_signatures/images#input\n",
    "        # The inputs pixel values are scaled between -1 and 1, sample-wise.\n",
    "        'preprocessor': tf.keras.applications.mobilenet_v2.preprocess_input,\n",
    "    },\n",
    "    {\n",
    "        'source': 'https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4',\n",
    "        'input_dim': 299,\n",
    "        # The inputs pixel values are scaled between -1 and 1, sample-wise.\n",
    "        'preprocessor': tf.keras.applications.inception_v3.preprocess_input,\n",
    "    },\n",
    "    {\n",
    "        'source': 'https://tfhub.dev/google/inaturalist/inception_v3/feature_vector/5',\n",
    "        'input_dim': 299,\n",
    "        # The inputs pixel values are scaled between -1 and 1, sample-wise.\n",
    "        'preprocessor': tf.keras.applications.inception_v3.preprocess_input,\n",
    "    },\n",
    "    {\n",
    "        'source': tf.keras.applications.Xception,\n",
    "        'input_dim': 299,\n",
    "        # The inputs pixel values are scaled between -1 and 1, sample-wise.\n",
    "        'preprocessor': tf.keras.applications.xception.preprocess_input,\n",
    "    },\n",
    "    {\n",
    "        'source': tf.keras.applications.resnet.ResNet101,\n",
    "        'input_dim': 224,\n",
    "        'preprocessor': tf.keras.applications.resnet50.preprocess_input,\n",
    "    },\n",
    "    {\n",
    "        'source': tf.keras.applications.ResNet50,\n",
    "        'input_dim': 224,\n",
    "        'preprocessor': tf.keras.applications.resnet50.preprocess_input,\n",
    "    },\n",
    "    {\n",
    "        'source': tf.keras.applications.InceptionResNetV2,\n",
    "        'input_dim': 299,\n",
    "        'preprocessor': tf.keras.applications.inception_resnet_v2.preprocess_input,\n",
    "    },\n",
    "    {\n",
    "        'source': tf.keras.applications.efficientnet_v2.EfficientNetV2B0,\n",
    "        'input_dim': 224,\n",
    "        # The preprocessing logic has been included in the EfficientNetV2\n",
    "        # model implementation. Users are no longer required to call this\n",
    "        # method to normalize the input data. This method does nothing and\n",
    "        # only kept as a placeholder to align the API surface between old\n",
    "        # and new version of model.\n",
    "        'preprocessor': tf.keras.applications.efficientnet_v2.preprocess_input,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d3d505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {}\n",
    "model[\"base\"] = base_models[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b66f3b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf8d406",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b652b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hdf_path = '/media/data/gbif/clean_data.h5'\n",
    "ds_hdf_key = 'media_merged_filtered-by-species_350pt'\n",
    "\n",
    "ds_col_filename = 'filename'\n",
    "ds_col_label = 'acceptedScientificName'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06565e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in source dataframe\n",
    "ds_df = pd.read_hdf( ds_hdf_path, ds_hdf_key )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8968969f",
   "metadata": {},
   "source": [
    "### Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfe4b81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label count: 2451\n",
      "Datapoint count: 665803\n"
     ]
    }
   ],
   "source": [
    "ds_classes = ds_df[ ds_col_label ].unique()\n",
    "print('Label count: %d' % len(ds_classes))\n",
    "print('Datapoint count: %d' % len(ds_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e823d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df_label_vc = ds_df[ ds_col_label ].value_counts()\n",
    "ds_df_label_vc = ds_df_label_vc.sort_values( ascending=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d014f0fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Amanita muscaria (L.) Lam.              1050\n",
       "Pisolithus arhizus (Scop.) Rauschert    1050\n",
       "Phellinus igniarius (L.) Quél.          1030\n",
       "Inonotus obliquus (Fr.) Pilát            968\n",
       "Collybiopsis (J.Schröt.) Earle, 1909     942\n",
       "Name: acceptedScientificName, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df_label_vc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97d31064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cheimonophyllum candidissimum (Berk. & M.A.Curtis) Singer                                    101\n",
       "Urocystis anemones (Pers.) G.Winter                                                          101\n",
       "Melanelixia subargentifera (Nyl.) O.Blanco, A.Crespo, Divakar, Essl., D.Hawksw. & Lumbsch    101\n",
       "Furia ithacensis (J.P.Kramer) Humber                                                         101\n",
       "Fistulina antarctica Speg.                                                                   101\n",
       "Name: acceptedScientificName, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df_label_vc.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c328f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_df_label_vc.hist( bins=50 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768b4828",
   "metadata": {},
   "source": [
    "Most classes have 350 datapoints because I thought I had downsampled the datasets with more than 350 datapoints down to 350 but it looks like there are some that were not affected by the transformation I performed. There are some classes with more than 350 datapoints somehow. Regardless, we will be downsampling them all to the lowest value_count (101)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2311854",
   "metadata": {},
   "source": [
    "### Dataset Transformation\n",
    "(downsample, upsample, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a86a1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least number of samples per class: 101\n"
     ]
    }
   ],
   "source": [
    "# Downsample to equal number of samples per class\n",
    "ds_df_label_vc_min = ds_df_label_vc.min()\n",
    "print('Least number of samples per class: %d' % ds_df_label_vc_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77f90178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Abortiporus biennis (Bull.) Singer                        101\n",
       "Perenniporia fraxinea (Bull.) Ryvarden                    101\n",
       "Peltula euploca (Ach.) Poelt ex Pišút                     101\n",
       "Penicillium digitatum (Pers.) Sacc.                       101\n",
       "Penicillium vulpinum (Cooke & Massee) Seifert & Samson    101\n",
       "                                                         ... \n",
       "Ganoderma curtisii (Berk.) Murrill                        101\n",
       "Ganoderma lobatum (Cooke) G.F.Atk.                        101\n",
       "Ganoderma lucidum (Curtis) P.Karst.                       101\n",
       "Ganoderma oregonense Murrill                              101\n",
       "Yarrumia coronata (Müll.Arg.) D.J.Galloway                101\n",
       "Name: acceptedScientificName, Length: 2451, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df_trans = ds_df.groupby( by=ds_col_label ).sample(n = ds_df_label_vc_min)\n",
    "ds_df_trans[ds_col_label].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88e070ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101    2451\n",
       "Name: acceptedScientificName, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df_trans[ds_col_label].value_counts().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d888fc08",
   "metadata": {},
   "source": [
    "We have transformed the original dataset through downsampling to produce a dataset where all classes have the same number of datapoints as the class with the least amount of datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32501e4",
   "metadata": {},
   "source": [
    "### Train, Validation, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d835ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_split_test = 0.05\n",
    "ds_split_val = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320ffe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df_train, ds_df_test = train_test_split(\n",
    "    ds_df_trans,\n",
    "    test_size = ds_split_test,\n",
    "    stratify = ds_df_trans[[ ds_col_label ]],\n",
    ")\n",
    "\n",
    "ds_df_train, ds_df_val = train_test_split(\n",
    "    ds_df_train,\n",
    "    test_size = ds_split_val,\n",
    "    stratify = ds_df_train[[ ds_col_label ]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db61aa5",
   "metadata": {},
   "source": [
    "### Input Data Pipeline Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024d2826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(\n",
    "    filename,\n",
    "):\n",
    "    img_raw = tf.io.read_file( filename )\n",
    "    img_tensor = tf.image.decode_image(\n",
    "        img_raw,\n",
    "        dtype = tf.dtypes.float32,\n",
    "        channels = 3,\n",
    "        expand_animations = False,\n",
    "    )\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd2f7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(\n",
    "    img_tensor,\n",
    "    input_dim,\n",
    "):\n",
    "    return tf.image.resize(\n",
    "        img_tensor,\n",
    "        [ input_dim, input_dim ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aee8112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(\n",
    "    img_tensor,\n",
    "    preprocessor,\n",
    "):\n",
    "    return preprocessor( img_tensor )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295408d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determines if preprocessing should be done in the IDP or in the model\n",
    "preprocessing_in_ds = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add21141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(\n",
    "    label,\n",
    "    classes,\n",
    "):\n",
    "    one_hot = label == classes\n",
    "    label_encoded = tf.argmax( one_hot )\n",
    "    return label_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2307d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(\n",
    "    img_tensor,\n",
    "    augmentation_func,\n",
    "):\n",
    "    return augmentation_func(img_tensor, training = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dbab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation function selection\n",
    "augmentation_functions = [\n",
    "    tf.keras.Sequential( [\n",
    "        tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        tf.keras.layers.RandomRotation(0.2),\n",
    "        \n",
    "    ] )\n",
    "]\n",
    "\n",
    "# set augmentation_func to None if no augmentation is desired\n",
    "augmentation_func = augmentation_functions[0]\n",
    "\n",
    "# Determines if data augmentation should be done in the IDP or in the model\n",
    "# Data augmentation will\n",
    "data_augmentation_in_ds = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ccc6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_idp(\n",
    "    filenames,\n",
    "    labels,\n",
    "    input_dim,\n",
    "    is_training = False,\n",
    "    batch_size = 32,\n",
    "    augmentation_func = None,\n",
    "):\n",
    "    ds = tf.data.Dataset.from_tensor_slices( (\n",
    "        filenames,\n",
    "        labels,\n",
    "    ) )\n",
    "    \n",
    "    # image loading\n",
    "    # (img_tensor, label)\n",
    "    ds = ds.map(\n",
    "        lambda filename, label: (\n",
    "            load_image(filename),\n",
    "            label,\n",
    "        ),\n",
    "        num_parallel_calls = AUTOTUNE,\n",
    "    )\n",
    "\n",
    "    # if isTraining, shuffle\n",
    "    # use a buffersize equal to the length of the dataset\n",
    "    if ( is_training ):\n",
    "        # ds = ds.shuffle( buffer_size = len(filenames) )\n",
    "        \n",
    "        # if isTraining and augmentation_func exists, use data augmentation\n",
    "        if ( augmentation_func ):\n",
    "            ds = ds.map(\n",
    "                lambda img_tensor, label: (\n",
    "                    data_augmentation(img_tensor, augmentation_func),\n",
    "                    label,\n",
    "                ),\n",
    "                num_parallel_calls = AUTOTUNE,\n",
    "            )\n",
    "    \n",
    "    # image resizing\n",
    "    # (img_tensor_resized, label)\n",
    "    ds = ds.map(\n",
    "        lambda img_tensor, label: (\n",
    "            resize( img_tensor, input_dim ),\n",
    "            label,\n",
    "        ),\n",
    "        num_parallel_calls = AUTOTUNE,\n",
    "    )\n",
    "    \n",
    "    # image preprocessing\n",
    "    # (img_tensor_resized_preprocessed, label)\n",
    "    if ( preprocessing_in_ds ):\n",
    "        ds = ds.map(\n",
    "            lambda img_tensor_resized, label: (\n",
    "                preprocessing(img_tensor_resized, model['base']['preprocessor']),\n",
    "                label,\n",
    "            ),\n",
    "            num_parallel_calls = AUTOTUNE,\n",
    "        )\n",
    "    \n",
    "    # label encoding\n",
    "    # (img_tensor_resized_preprocessed, label_encoded)\n",
    "    ds = ds.map(\n",
    "        lambda img_tensor_resized_preprocessed, label: (\n",
    "            img_tensor_resized_preprocessed,\n",
    "            encode_label( label, ds_classes ),\n",
    "        ),\n",
    "        num_parallel_calls = AUTOTUNE,\n",
    "    )\n",
    "    \n",
    "    # Batch\n",
    "    ds = ds.batch( batch_size )\n",
    "    \n",
    "    # Prefetch\n",
    "    ds = ds.prefetch( buffer_size = AUTOTUNE )\n",
    "    \n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDP creation\n",
    "ds_idp_train = make_idp(\n",
    "    ds_df_train[ ds_col_filename ].values,\n",
    "    ds_df_train[ ds_col_label ].values,\n",
    "    input_dim = model['base']['input_dim'],\n",
    "    is_training = True,\n",
    "    batch_size = batch_size,\n",
    "    augmentation_func = augmentation_func if ( augmentation_func and data_augmentation_in_ds ) else None,\n",
    ")\n",
    "\n",
    "ds_idp_val = make_idp(\n",
    "    ds_df_val[ ds_col_filename ].values,\n",
    "    ds_df_val[ ds_col_label ].values,\n",
    "    input_dim = model['base']['input_dim'],\n",
    "    is_training = False,\n",
    "    batch_size = batch_size,\n",
    "    # turned off by is_training = False anyway...\n",
    "    augmentation_func = None,\n",
    ")\n",
    "\n",
    "ds_idp_test = make_idp(\n",
    "    ds_df_test[ ds_col_filename ].values,\n",
    "    ds_df_test[ ds_col_label ].values,\n",
    "    input_dim = model['base']['input_dim'],\n",
    "    is_training = False,\n",
    "    batch_size = batch_size,\n",
    "    # turned off by is_training = False anyway...\n",
    "    augmentation_func = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7829410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_label_mapping(\n",
    "#     label_mapping,\n",
    "#     file_path = './label_mapping.json',\n",
    "# ):\n",
    "#     with open( file_path, 'w' ) as f:\n",
    "#         json.dump( label_mapping, f, indent = 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de845ab",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aa7788",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0c6376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a name that accurately describes the model building function or\n",
    "# the tfhub model (by url) that was passed\n",
    "def get_model_name( model_handle ):\n",
    "\n",
    "    if callable(model_handle):\n",
    "        return f'keras.applications/{model_handle.__name__}'\n",
    "    else:\n",
    "        split = model_handle.split('/')\n",
    "        return f'tfhub/{split[-5]}.{split[-4]}.{split[-3]}'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5396f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize full model\n",
    "model[\"model\"] = tf.keras.Sequential( name = \"full_model\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979580f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is assumed to be resized and preprocessed correctly from input dataset pipeline (idp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481821a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate base_model layer\n",
    "def gen_base_model_layer(\n",
    "    name,\n",
    "    source,\n",
    "    input_dim,\n",
    "    trainable = False,\n",
    "):\n",
    "    # If model_handle is a model building function, use that function\n",
    "    if callable( source ):\n",
    "        base_model = source(\n",
    "            include_top = False,\n",
    "            input_shape = ( input_dim, input_dim ) + (3,),\n",
    "            weights = 'imagenet',\n",
    "            # pooling = 'avg',\n",
    "        )\n",
    "\n",
    "    # otherwise build a layer from the tfhub url that was passed as a string\n",
    "    else:\n",
    "        base_model = hub.KerasLayer(\n",
    "            source,\n",
    "            input_shape = ( input_dim, input_dim ) + (3,),\n",
    "            name = name,\n",
    "        )\n",
    "    \n",
    "    base_model.trainable = trainable\n",
    "\n",
    "    return base_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab9826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add base model to full_model\n",
    "model[\"model\"].add( gen_base_model_layer(\n",
    "    name = get_model_name( model['base']['source'] ),\n",
    "    source = model['base']['source'],\n",
    "    input_dim = model['base']['input_dim'],\n",
    "    trainable = True,\n",
    ") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c49a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate classifier\n",
    "def gen_classifier_model_layer(\n",
    "    num_classes,\n",
    "    dropout,\n",
    "):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(\n",
    "        layers.Dense(\n",
    "            num_classes,\n",
    "            # activation = 'softmax',\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(\n",
    "        layers.Dropout(dropout)\n",
    "    )\n",
    "\n",
    "    model.add(\n",
    "        layers.Activation(\"softmax\", dtype=\"float32\")\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1d45ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add classifier model to full_model\n",
    "# TODO allow selection between different classification models\n",
    "model['model'].add( gen_classifier_model_layer(\n",
    "    num_classes = len( ds_classes ),\n",
    "    dropout = classifier_dropout,\n",
    ") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65bf126",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d60b33",
   "metadata": {},
   "source": [
    "## Run Results Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3401105",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunLogging():\n",
    "\n",
    "    hdf_key = \"runs\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir = \"./\",\n",
    "        hdf_filename = \"runs.h5\",\n",
    "    ):\n",
    "        self.data_dir = data_dir\n",
    "        self.filename =  os.path.join( data_dir, hdf_filename )\n",
    "        self.df = self.load_metadata()\n",
    "\n",
    "    def load_metadata(self):\n",
    "        if ( os.path.exists(self.filename) ):\n",
    "            return pd.read_hdf( self.filename, self.hdf_key )\n",
    "        else: return pd.DataFrame()\n",
    "\n",
    "    def save_df(self):\n",
    "        self.df.to_hdf(self.filename, self.hdf_key)\n",
    "\n",
    "    def add_run(self, params, log_dir, time, scores):\n",
    "\n",
    "        cols = {\n",
    "            **params,\n",
    "            'time': time,\n",
    "            'log_dir': log_dir,\n",
    "            'scores.loss': scores[0],\n",
    "            'scores.accuracy': scores[1],\n",
    "            'scores.top3': scores[2],\n",
    "            'scores.top10': scores[3],\n",
    "        }\n",
    "\n",
    "        new_run = pd.DataFrame([cols])\n",
    "\n",
    "        self.df = pd.concat([self.df, new_run])\n",
    "        self.save_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb6dd65",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4286d70",
   "metadata": {},
   "source": [
    "## Build and run all models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b6575",
   "metadata": {},
   "source": [
    "* Note regarding `thawed_base_model_layers` and full model architecture ([reference](https://stackoverflow.com/questions/64227483/what-is-the-right-way-to-gradually-unfreeze-layers-in-neural-network-while-learn))\n",
    "![image](https://i.stack.imgur.com/JLJqv.png)\n",
    "* [Another great reference](https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a02943f",
   "metadata": {},
   "source": [
    "# Training Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed107505",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "max_epochs = 20\n",
    "\n",
    "### Optimizer\n",
    "learning_rate = 0.0001\n",
    "\n",
    "### Loss\n",
    "label_smoothing = 0.1\n",
    "\n",
    "# TODO: allow loading of model weights from previous run\n",
    "load_weights = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b3f0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Logging\n",
    "run_logs = RunLogging(\n",
    "    data_dir = runs_dir,\n",
    ")\n",
    "\n",
    "base_model_id = get_model_name( model['base']['source'] )\n",
    "run_log_dir = os.path.join( run_logs.data_dir, 'gbif', base_model_id )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef2fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "# Sparse vs non-sparse CCE https://www.kaggle.com/general/197993\n",
    "model[\"model\"].compile(\n",
    "    optimizer = tf.keras.optimizers.Adam( learning_rate = learning_rate ),\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy( from_logits = False ),\n",
    "    # loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "        # from_logits=True,\n",
    "    #     label_smoothing = label_smoothing,\n",
    "    # ),\n",
    "    metrics = [\n",
    "        'accuracy',\n",
    "        # tf.keras.metrics.AUC(),\n",
    "        tf.keras.metrics.SparseCategoricalCrossentropy(),\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(\n",
    "            k = 3,\n",
    "            name=\"Top3\",\n",
    "        ),\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(\n",
    "            k = 10,\n",
    "            name=\"Top10\",\n",
    "        ),\n",
    "        # tf.keras.metrics.CategoricalCrossentropy(),            \n",
    "        # tf.keras.metrics.TopKCategoricalAccuracy( k=3, name=\"Top3\" ),\n",
    "        # tf.keras.metrics.TopKCategoricalAccuracy( k=10, name=\"Top10\" ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f6e27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard logs\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir = run_path,\n",
    "    histogram_freq = 1,\n",
    ")\n",
    "\n",
    "# Early stopping\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    # monitor='val_sparse_categorical_accuracy',\n",
    "    monitor = 'val_loss',\n",
    "    patience = 5,\n",
    "    min_delta = 0.01,\n",
    ")\n",
    "\n",
    "# Model Checkpoints for saving best model weights\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    os.path.join(run_log_dir, 'best_model' ),\n",
    "    save_best_only = True,\n",
    "    monitor = 'val_loss',\n",
    "    # mode = 'min', # should be chosen correctly based on monitor value\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8158dd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "history = model[\"model\"].fit(\n",
    "    ds_idp_train,\n",
    "    validation_data = ds_idp_val,\n",
    "    epochs = max_epochs,\n",
    "    callbacks = [\n",
    "        tensorboard_callback,\n",
    "        early_stopping_callback,\n",
    "        model_checkpoint_callback,\n",
    "    ],\n",
    "    # validation_freq=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eac5a41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-fungi]",
   "language": "python",
   "name": "conda-env-.conda-fungi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
