{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518a0c58",
   "metadata": {},
   "source": [
    "# Model Generation for GBIF Fungi Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b411778a",
   "metadata": {},
   "source": [
    "## References\n",
    "* [Transfer Learning with Hub](https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub)\n",
    "* [`tf.keras.utils.image_dataset_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory)\n",
    "* [Limiting GPU Memory Growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a285c8a1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2c0f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from keras.utils.layer_utils import count_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a639289",
   "metadata": {},
   "source": [
    "### Limit GPU memory allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16845159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_memory_growth(limit=True):\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, limit)\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10a47939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "limit_memory_growth()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106e1972",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf8d406",
   "metadata": {},
   "source": [
    "## Enumerate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b652b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "flowers_dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "flowers_data_dir = tf.keras.utils.get_file('flower_photos', origin=flowers_dataset_url, untar=True)\n",
    "flowers_data_dir = pathlib.Path(flowers_data_dir)\n",
    "\n",
    "datasets = [\n",
    "    ('CUB-200-2011', '/mnt/cub/CUB_200_2011/images'),\n",
    "    ('flowers', flowers_data_dir),\n",
    "    (\n",
    "        'GBIF_fungi',\n",
    "        '/mnt/gbif/media',\n",
    "        # '/mnt/gbif/media',\n",
    "        # (\n",
    "        #     '/mnt/gbif/clean_data.h5',\n",
    "        #     'media_merged_filtered-by-species_350pt',\n",
    "        #     'map',\n",
    "        #     'labels',\n",
    "        # ),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c9c943",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd46f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dataset object from a directory of images\n",
    "def build_dataset(\n",
    "    dataset,\n",
    "    image_size,\n",
    "    preprocess_input = None,\n",
    "    batch_size = 64,\n",
    "):\n",
    "    folder_labels = True\n",
    "    labels = 'infer'\n",
    "    if len(dataset) > 2:\n",
    "        folder_labels = False\n",
    "        \n",
    "    if not folder_labels:\n",
    "        labels_df = pd.read_hdf(dataset[2][0], dataset[2][3])\n",
    "        labels = labels_df['acceptedScientificName'].tolist()\n",
    "        print(f'# Labels: {len(labels)}')\n",
    "   \n",
    "    train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        dataset[1],\n",
    "        batch_size = batch_size,\n",
    "        validation_split = 0.05,\n",
    "        image_size = image_size,\n",
    "        subset = \"both\",\n",
    "        shuffle = True, # default but here for clarity\n",
    "        seed = 42,\n",
    "        label_mode = \"categorical\", # enables one-hot encoding (use 'int' for sparse_categorical_crossentropy loss)\n",
    "        # labels = labels, # need to default to 'infer', not None\n",
    "    )\n",
    "    \n",
    "    # Retrieve class names\n",
    "    # (can't do this after converting to PrefetchDataset?)\n",
    "    class_names = train_ds.class_names\n",
    "    \n",
    "    # Prefetch images\n",
    "    # train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    # val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # apply preprocessing function\n",
    "    train_ds = train_ds.map(\n",
    "        lambda x, y: (preprocess_input(x), y),\n",
    "        num_parallel_calls = 16,\n",
    "    )\n",
    "    val_ds = val_ds.map(\n",
    "        lambda x, y: (preprocess_input(x), y),\n",
    "        num_parallel_calls = 16,\n",
    "    )\n",
    "    \n",
    "    return (train_ds, val_ds, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf40cdbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1de845ab",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf56f3",
   "metadata": {},
   "source": [
    "## Enumerate Models to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee979c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of tuples describing the models to be tested\n",
    "# in the form: (model_handle, input_image_size, preprocessing_function)\n",
    "# where the model_handle is a model building function or a url to a tfhub feature model\n",
    "base_models_metadata = [\n",
    "    (\n",
    "        'https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4',\n",
    "        224,\n",
    "        # https://www.tensorflow.org/hub/common_signatures/images#input\n",
    "        # The inputs pixel values are scaled between -1 and 1, sample-wise.\n",
    "        tf.keras.applications.mobilenet_v2.preprocess_input,\n",
    "    ),\n",
    "    (\n",
    "        'https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4',\n",
    "        299,\n",
    "        # The inputs pixel values are scaled between -1 and 1, sample-wise.\n",
    "        tf.keras.applications.inception_v3.preprocess_input,\n",
    "    ),\n",
    "    (\n",
    "        'https://tfhub.dev/google/inaturalist/inception_v3/feature_vector/5',\n",
    "        299,\n",
    "        # The inputs pixel values are scaled between -1 and 1, sample-wise.\n",
    "        tf.keras.applications.inception_v3.preprocess_input,\n",
    "    ),\n",
    "    (\n",
    "        tf.keras.applications.Xception,\n",
    "        299,\n",
    "        # The inputs pixel values are scaled between -1 and 1, sample-wise.\n",
    "        tf.keras.applications.xception.preprocess_input,\n",
    "    ),\n",
    "    (\n",
    "        tf.keras.applications.resnet.ResNet101,\n",
    "        224,\n",
    "        tf.keras.applications.resnet50.preprocess_input,\n",
    "    ),\n",
    "    (\n",
    "        tf.keras.applications.ResNet50,\n",
    "        224,\n",
    "        tf.keras.applications.resnet50.preprocess_input,\n",
    "    ),\n",
    "    (\n",
    "        tf.keras.applications.InceptionResNetV2,\n",
    "        299,\n",
    "        tf.keras.applications.inception_resnet_v2.preprocess_input,\n",
    "    ),\n",
    "    (\n",
    "        tf.keras.applications.efficientnet_v2.EfficientNetV2B0,\n",
    "        224,\n",
    "        # The preprocessing logic has been included in the EfficientNetV2\n",
    "        # model implementation. Users are no longer required to call this\n",
    "        # method to normalize the input data. This method does nothing and\n",
    "        # only kept as a placeholder to align the API surface between old\n",
    "        # and new version of model.\n",
    "        tf.keras.applications.efficientnet_v2.preprocess_input,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a0c6376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a name that accurately describes the model building function or\n",
    "# the tfhub model (by url) that was passed\n",
    "def get_model_name( model_handle ):\n",
    "    \n",
    "    if callable(model_handle):\n",
    "        return f'keras.applications/{model_handle.__name__}'\n",
    "    else:\n",
    "        split = model_handle.split('/')\n",
    "        return f'tfhub/{split[-5]}.{split[-4]}.{split[-3]}'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aa7788",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a06cf38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FullModel(tf.keras.Sequential):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model_metadata,\n",
    "        num_classes,\n",
    "        dropout,\n",
    "        thawed_base_model_layers,\n",
    "        image_size,\n",
    "        data_augmentation = False,\n",
    "    ):\n",
    "        \n",
    "        input_layer = layers.Input(\n",
    "            shape = image_size + (3,)\n",
    "        )\n",
    "        \n",
    "        base_model = base_model = self.build_base_model(\n",
    "            model_handle,\n",
    "            input_tensor=input_layer,\n",
    "            thawed_base_model_layers = thawed_base_model_layers,\n",
    "        )(input_layer)\n",
    "        \n",
    "        # avg = layers.GlobalAveragePooling2D()(base_model)\n",
    "        # dropout_layer = layers.Dropout(dropout)(base_model)\n",
    "        x = layers.Dense(num_classes, name='dense_logits')(dropout_layer)\n",
    "        output = layers.Activation('softmax', dtype='float32', name='predictions')(x)\n",
    "        \n",
    "        super().__init__(\n",
    "            name = \"full_model\",\n",
    "            inputs = base_model.input,\n",
    "            outputs = output,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def build_data_augmentation():\n",
    "        data_augmentation = keras.Sequential([\n",
    "            layers.RandomFlip(\n",
    "                \"horizontal\",\n",
    "            ),\n",
    "            layers.RandomRotation(0.1),\n",
    "            layers.RandomZoom(0.1),\n",
    "        ], name = \"data_augmentation\")\n",
    "\n",
    "        return data_augmentation\n",
    "\n",
    "    @staticmethod\n",
    "    def build_base_model(\n",
    "        base_model_handle,\n",
    "        input_tensor,\n",
    "        thawed_base_model_layers = 0,\n",
    "        name=\"base_model\",\n",
    "    ):\n",
    "        # If model_handle is a model building function, use that function\n",
    "        if callable(base_model_handle):\n",
    "            base_model = model_handle(\n",
    "                include_top=False,\n",
    "                input_tensor=input_tensor,\n",
    "                weights='imagenet',\n",
    "                pooling = 'avg',\n",
    "            )\n",
    "\n",
    "        # otherwise build a layer from the tfhub url that was passed as a string\n",
    "        else:\n",
    "            base_model = hub.KerasLayer(\n",
    "                base_model_handle,\n",
    "                # input_shape=[],\n",
    "                name=name,\n",
    "            )\n",
    "            \n",
    "        # Freeze specified # of layers\n",
    "        # FullModel.freeze_base_model(base_model, thawed_base_model_layers)\n",
    "        base_model.trainable = True\n",
    "\n",
    "\n",
    "        # Print Base model weights\n",
    "        print(\"\\nBase Model:\")\n",
    "        FullModel.print_weight_counts(base_model)\n",
    "\n",
    "        return base_model\n",
    "    \n",
    "    # Freeze base model?\n",
    "    @staticmethod\n",
    "    def freeze_base_model(\n",
    "        base_model,\n",
    "        thawed_base_model_layers = 0,\n",
    "    ):\n",
    "        print(\"Thawing...\", thawed_base_model_layers)\n",
    "        \n",
    "        print(base_model)\n",
    "        print(base_model.summary())\n",
    "        \n",
    "        if thawed_base_model_layers == 0:\n",
    "            base_model.trainable = False\n",
    "        elif thawed_base_model_layers > 0:\n",
    "            for layer in base_model.layers[(-1*thawed_base_model_layers):]:\n",
    "                layer.trainable = False\n",
    "        else:\n",
    "            base_model.trainable = True\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_classifier_model(\n",
    "        num_classes,\n",
    "        dropout,\n",
    "    ):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                num_classes,\n",
    "                # activation = 'softmax',\n",
    "            )\n",
    "        )\n",
    "\n",
    "        model.add(\n",
    "            layers.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        model.add(\n",
    "            layers.Activation(\"softmax\", dtype=\"float32\")\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    # Print model weight counts\n",
    "    @staticmethod\n",
    "    def print_weight_counts(model):\n",
    "        print(f'Non-trainable weights: {count_params(model.non_trainable_weights)}')\n",
    "        print(f'Trainable weights: {count_params(model.trainable_weights)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65bf126",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d60b33",
   "metadata": {},
   "source": [
    "## Run Results Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3401105",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunLogging():\n",
    "\n",
    "    hdf_key = \"runs\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir = \"./\",\n",
    "        hdf_filename = \"runs.h5\",\n",
    "    ):\n",
    "        self.data_dir = data_dir\n",
    "        self.filename =  os.path.join( data_dir, hdf_filename )\n",
    "        self.df = self.load_metadata()\n",
    "\n",
    "    def load_metadata(self):\n",
    "        if ( os.path.exists(self.filename) ):\n",
    "            return pd.read_hdf( self.filename, self.hdf_key )\n",
    "        else: return pd.DataFrame()\n",
    "\n",
    "    def save_df(self):\n",
    "        self.df.to_hdf(self.filename, self.hdf_key)\n",
    "\n",
    "    def add_run(self, params, log_dir, time, scores):\n",
    "\n",
    "        cols = {\n",
    "            **params,\n",
    "            'time': time,\n",
    "            'log_dir': log_dir,\n",
    "            'scores.loss': scores[0],\n",
    "            'scores.accuracy': scores[1],\n",
    "            'scores.top3': scores[2],\n",
    "            'scores.top10': scores[3],\n",
    "        }\n",
    "\n",
    "        new_run = pd.DataFrame([cols])\n",
    "\n",
    "        self.df = pd.concat([self.df, new_run])\n",
    "        self.save_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb6dd65",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4286d70",
   "metadata": {},
   "source": [
    "## Build and run all models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b6575",
   "metadata": {},
   "source": [
    "* Note regarding `thawed_base_model_layers` and full model architecture ([reference](https://stackoverflow.com/questions/64227483/what-is-the-right-way-to-gradually-unfreeze-layers-in-neural-network-while-learn))\n",
    "![image](https://i.stack.imgur.com/JLJqv.png)\n",
    "* [Another great reference](https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed107505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "## Dataset Hyperparameters\n",
    "dataset = datasets[2] # GBIF_fungi dataset\n",
    "batch_size = 128\n",
    "\n",
    "## Full Model Hyperparameters\n",
    "\n",
    "### Input Model Hyperparameters\n",
    "data_augmentation = False\n",
    "\n",
    "### Base Model Hyperparameters\n",
    "thawed_base_model_layers = 0 # base_model completely frozen\n",
    "# thawed_base_model_layers = n # last 'n' layers of base_model unfrozen\n",
    "# thawed_base_model_layers = -1 # all base_model layers unfrozen\n",
    "\n",
    "### Classifier Model Hyperparameters\n",
    "classifier_dropout = 0.33\n",
    "\n",
    "## Training Hyperparameters\n",
    "max_epochs = 20\n",
    "\n",
    "### Optimizer Function Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "\n",
    "### Loss function hyperparameters\n",
    "label_smoothing = 0.1\n",
    "\n",
    "# use logits?\n",
    "\n",
    "# vary classifier architecture?\n",
    "\n",
    "\n",
    "load_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9b7bde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 665803 files belonging to 2451 classes.\n",
      "Using 632513 files for training.\n",
      "Using 33290 files for validation.\n",
      "WARNING:tensorflow:From /home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "base_model_metadata = base_models_metadata[3]\n",
    "\n",
    "model_handle, input_dimension, preprocess_input = base_model_metadata\n",
    "\n",
    "image_size = (input_dimension, input_dimension)\n",
    "\n",
    "train_ds, val_ds, class_names = build_dataset(\n",
    "    dataset,\n",
    "    batch_size = batch_size,\n",
    "    image_size = image_size,\n",
    "    preprocess_input = preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7e25da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_classnames(class_names, './')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c81cb453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_name(name):\n",
    "    \n",
    "    split = name.split('.')\n",
    "    \n",
    "    return {\n",
    "        \"index\": int(split[0]),\n",
    "        \"name\": split[1].replace('_', ' '),\n",
    "    }\n",
    "\n",
    "def save_classnames(class_names, log_dir):\n",
    "\n",
    "    with open( os.path.join( log_dir, 'classes.json'), 'w' )  as out:\n",
    "        json.dump(class_names, out, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03b3f0ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RunLogging' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run Logging\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m run_logs \u001b[38;5;241m=\u001b[39m \u001b[43mRunLogging\u001b[49m(\n\u001b[1;32m      3\u001b[0m     data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels_cub_02_logs\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# for each base model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#for base_model_metadata in base_models_metadata:\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# base_model_metadata = base_models_metadata[2]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m base_model_metadata \u001b[38;5;241m=\u001b[39m base_models_metadata[\u001b[38;5;241m3\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RunLogging' is not defined"
     ]
    }
   ],
   "source": [
    "# Run Logging\n",
    "run_logs = RunLogging(\n",
    "    data_dir = f'models_cub_02_logs',\n",
    ")\n",
    "\n",
    "# for each base model\n",
    "#for base_model_metadata in base_models_metadata:\n",
    "# base_model_metadata = base_models_metadata[2]\n",
    "base_model_metadata = base_models_metadata[3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab9e4dea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 665803 files belonging to 2451 classes.\n",
      "Using 632513 files for training.\n",
      "Using 33290 files for validation.\n",
      "WARNING:tensorflow:From /home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n",
      "\n",
      "Base Model:\n",
      "Non-trainable weights: 34432\n",
      "Trainable weights: 21768352\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"global_average_pooling2d\" is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: (None, 2048)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m         model \u001b[38;5;241m=\u001b[39m old_model\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_new_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Logging\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mrun_log_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m, in \u001b[0;36mbuild_new_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_new_model\u001b[39m():\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Build model\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mFullModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_model_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclassifier_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthawed_base_model_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthawed_base_model_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_augmentation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_augmentation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Compile model\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Sparse vs non-sparse CCE https://www.kaggle.com/general/197993\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     27\u001b[0m         optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate),\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;66;03m# loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m         ],\n\u001b[1;32m     41\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m, in \u001b[0;36mFullModel.__init__\u001b[0;34m(self, base_model_metadata, num_classes, dropout, thawed_base_model_layers, image_size, data_augmentation)\u001b[0m\n\u001b[1;32m     13\u001b[0m input_layer \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mInput(\n\u001b[1;32m     14\u001b[0m     shape \u001b[38;5;241m=\u001b[39m image_size \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m3\u001b[39m,)\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m base_model \u001b[38;5;241m=\u001b[39m base_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_base_model(\n\u001b[1;32m     18\u001b[0m     model_handle,\n\u001b[1;32m     19\u001b[0m     input_tensor\u001b[38;5;241m=\u001b[39minput_layer,\n\u001b[1;32m     20\u001b[0m     thawed_base_model_layers \u001b[38;5;241m=\u001b[39m thawed_base_model_layers,\n\u001b[1;32m     21\u001b[0m )(input_layer)\n\u001b[0;32m---> 23\u001b[0m avg \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGlobalAveragePooling2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m dropout_layer \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDropout(dropout)(avg)\n\u001b[1;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDense(num_classes, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdense_logits\u001b[39m\u001b[38;5;124m'\u001b[39m)(dropout_layer)\n",
      "File \u001b[0;32m~/.conda/envs/fungi/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.conda/envs/fungi/lib/python3.10/site-packages/keras/engine/input_spec.py:232\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    230\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m shape\u001b[38;5;241m.\u001b[39mrank\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m spec\u001b[38;5;241m.\u001b[39mndim:\n\u001b[0;32m--> 232\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    233\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    234\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    235\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         )\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmax_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"global_average_pooling2d\" is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: (None, 2048)"
     ]
    }
   ],
   "source": [
    "\n",
    "model_handle, input_dimension, preprocess_input = base_model_metadata\n",
    "\n",
    "image_size = (input_dimension, input_dimension)\n",
    "\n",
    "# Build dataset/pipeline\n",
    "train_ds, val_ds, class_names = build_dataset(\n",
    "    dataset,\n",
    "    batch_size = batch_size,\n",
    "    image_size = image_size,\n",
    "    preprocess_input = preprocess_input,\n",
    ")\n",
    "\n",
    "def build_new_model():\n",
    "    # Build model\n",
    "    model = FullModel(\n",
    "        base_model_metadata,\n",
    "        len(class_names),\n",
    "        classifier_dropout,\n",
    "        thawed_base_model_layers = thawed_base_model_layers,\n",
    "        image_size = image_size,\n",
    "        data_augmentation = data_augmentation,\n",
    "    )\n",
    "\n",
    "    # Compile model\n",
    "    # Sparse vs non-sparse CCE https://www.kaggle.com/general/197993\n",
    "    model.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        # loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(\n",
    "            # from_logits=True,\n",
    "            label_smoothing = label_smoothing,\n",
    "        ),\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.CategoricalAccuracy(),\n",
    "            tf.keras.metrics.CategoricalCrossentropy(),\n",
    "            tf.keras.metrics.AUC(),\n",
    "            tf.keras.metrics.TopKCategoricalAccuracy(k=3, name=\"Top3\"),\n",
    "            tf.keras.metrics.TopKCategoricalAccuracy(k=10, name=\"Top10\"),\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base_model_id = get_model_name(model_handle)\n",
    "run_log_dir = os.path.join( run_logs.data_dir, dataset[0], base_model_id )\n",
    "\n",
    "saved_model_dir = os.path.join(run_log_dir + '.00', 'best_model')\n",
    "\n",
    "if os.path.isdir(saved_model_dir):\n",
    "    print(saved_model_dir)\n",
    "    print(\"Saved Model Exists, loading.\")\n",
    "    print(saved_model_dir)\n",
    "    old_model = tf.keras.models.load_model(saved_model_dir)\n",
    "    if ( load_weights ):\n",
    "        print(\"Loading weights from saved model\")\n",
    "        model = build_new_model()\n",
    "        model.load_weights(saved_model_dir)\n",
    "    else:\n",
    "        model = old_model\n",
    "else:\n",
    "    model = build_new_model()\n",
    "\n",
    "# Logging\n",
    "print(f'\\n{run_log_dir}')\n",
    "\n",
    "# Tensorboard logs\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir = run_log_dir,\n",
    "    histogram_freq = 1,\n",
    ")\n",
    "\n",
    "# Early stopping\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    # monitor='val_sparse_categorical_accuracy',\n",
    "    monitor = 'val_loss',\n",
    "    patience = 5,\n",
    "    min_delta = 0.01,\n",
    ")\n",
    "\n",
    "# Model Checkpoints for saving best model weights\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    os.path.join(run_log_dir, 'best_model' ),\n",
    "    save_best_only = True,\n",
    "    monitor = 'val_loss',\n",
    "    # mode = 'min', # should be chosen correctly based on monitor value\n",
    ")\n",
    "\n",
    "best_batch = (0, float('inf'))\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=max_epochs,\n",
    "    callbacks=[\n",
    "        tensorboard_callback,\n",
    "        # early_stopping_callback,\n",
    "        model_checkpoint_callback,\n",
    "        tf.keras.callbacks.LambdaCallback(\n",
    "            on_epoch_end = lambda batch, logs: print((batch, logs['loss']) if logs['loss'] < best_batch[1] else best_batch),\n",
    "        )\n",
    "    ],\n",
    "    # validation_freq=2,\n",
    ")\n",
    "\n",
    "print(history)\n",
    "print(max(history.history['val_loss']))\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "save_classnames(class_names, run_log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ddc2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"dataset\": dataset[0],\n",
    "    \"base_model\": base_model_metadata[:2],\n",
    "    \"batch_size\": batch_size,\n",
    "    \"data_augmentation\": data_augmentation,\n",
    "    \"thawed_base_model_layers\": thawed_base_model_layers,\n",
    "    \"classifier_dropout\": classifier_dropout,\n",
    "    \"max_epochs\": max_epochs,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"label_smoothing\": label_smoothing,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00575ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hparams(hparams, log_dir):\n",
    "    with open( os.path.join( log_dir, 'hparams.json'), 'w' )  as out:\n",
    "        json.dump(hparams, out, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703e458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_hparams(hparams, run_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b93ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb3fbaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
