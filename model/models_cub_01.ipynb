{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518a0c58",
   "metadata": {},
   "source": [
    "# Models Exploration using CUB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b411778a",
   "metadata": {},
   "source": [
    "## References\n",
    "* [Transfer Learning with Hub](https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub)\n",
    "* [`tf.keras.utils.image_dataset_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory)\n",
    "* [Limiting GPU Memory Growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a285c8a1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2c0f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeaabc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_memory_growth(limit=True):\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, limit)\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5bbc7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "limit_memory_growth()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3e8d4f",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47fe9ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(\n",
    "    image_batch,\n",
    "    predicted_class_names,\n",
    "):\n",
    "    plt.figure(figsize=(10,9))\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    for n in range(30):\n",
    "        plt.subplot(6,5,n+1)\n",
    "        plt.imshow(image_batch[n])\n",
    "        plt.title(predicted_class_names[n])\n",
    "        plt.axis('off')\n",
    "    _ = plt.suptitle(\"Predictions\")\n",
    "\n",
    "def plot_images(\n",
    "    ds,\n",
    "    class_names,\n",
    "):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for images, labels in ds.tjake(1):\n",
    "        for i in range(9):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "            plt.title(class_names[labels[i]])\n",
    "            plt.axis(\"off\")\n",
    "    \n",
    "def get_timestamp():\n",
    "    return datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c9c943",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd46f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fun( x, y, preprocess_input ):\n",
    "    print(type(x))\n",
    "    print(x.shape)\n",
    "    print(type(y))\n",
    "    print(y.shape)\n",
    "    return (x, y)\n",
    "\n",
    "def build_dataset(\n",
    "    data_dir = '/mnt/cub/CUB_200_2011/images',\n",
    "    batch_size = 64,\n",
    "    image_size = (299,299),\n",
    "    preprocess_input = None,\n",
    "    # normalization = True,\n",
    "):\n",
    "   \n",
    "    train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        batch_size = batch_size,\n",
    "        validation_split = 0.2,\n",
    "        image_size = image_size,\n",
    "        subset = \"both\",\n",
    "        shuffle = True, # default but here for clarity\n",
    "        seed=42,\n",
    "    )\n",
    "    '''\n",
    "    datagen = ImageDataGenerator(\n",
    "        preprocessing_function = preprocess_input,\n",
    "        validation_split=0.2,\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.15,\n",
    "        zoom_range=0.1,\n",
    "        channel_shift_range=10.,\n",
    "        horizontal_flip=True,\n",
    "    )\n",
    "    \n",
    "    train_ds = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=(299,299),\n",
    "        batch_size=batch_size,\n",
    "        subset='training',\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    val_ds = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=(299,299),\n",
    "        batch_size=batch_size,\n",
    "        subset='validation',\n",
    "        shuffle=True,\n",
    "    )\n",
    "    '''\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Use model specific preprocessing function\n",
    "    if preprocess_input:\n",
    "        train_ds.map(lambda x, y: (preprocess_input(x), y))\n",
    "        val_ds.map(lambda x, y: (preprocess_input(x), y))\n",
    "    else:\n",
    "        # normalization_layer = layers.Rescaling(\n",
    "        #     1./255,\n",
    "        #     name=\"normalization_layer\",\n",
    "        # )\n",
    "        # train_ds.map(lambda x, y: (normalization_layer(x)-0.5, y))\n",
    "        # val_ds.map(lambda x, y: (normalization_layer(x)-0.5, y))\n",
    "        pass\n",
    "    '''\n",
    "    \n",
    "    # Retrieve class names\n",
    "    # (can't do this after converting to PrefetchDataset?)\n",
    "    class_names = train_ds.class_names\n",
    "    \n",
    "    # Prefetch images\n",
    "    train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # apply preprocessing function\n",
    "    train_ds.map(\n",
    "        lambda x, y: preprocess_fun(x, y, preprocess_input)\n",
    "    )\n",
    "    \n",
    "    return (train_ds, val_ds, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf56f3",
   "metadata": {},
   "source": [
    "## Enumerate Models to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee979c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models_metadata = [\n",
    "    # ('https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4', 224),\n",
    "    # ('https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4', 299),\n",
    "    # ('https://tfhub.dev/google/inaturalist/inception_v3/feature_vector/5', 299),\n",
    "    (tf.keras.applications.Xception, 299, tf.keras.applications.xception.preprocess_input),\n",
    "    # (tf.keras.applications.resnet.ResNet101, 224),\n",
    "    # (tf.keras.applications.ResNet50, 224),\n",
    "    # (tf.keras.applications.InceptionResNetV2, 299),\n",
    "    # (tf.keras.applications.efficientnet_v2.EfficientNetV2B0, 224)\n",
    "]\n",
    "\n",
    "def get_model_name( model_handle ):\n",
    "    \n",
    "    if callable(model_handle):\n",
    "        return f'keras.applications.{model_handle.__name__}'\n",
    "    else:\n",
    "        split = model_handle.split('/')\n",
    "        return f'{split[-5]}.{split[-4]}.{split[-3]}'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aa7788",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a06cf38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_model_layer(\n",
    "    model_handle,\n",
    "    name=\"base_model_layer\",\n",
    "):\n",
    "    if callable(model_handle):\n",
    "        base_model_layer = model_handle(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            pooling = 'avg',\n",
    "            # trainable=False,\n",
    "        )\n",
    "    else:\n",
    "        base_model_layer = hub.KerasLayer(\n",
    "            model_handle,\n",
    "            name=name,\n",
    "            trainable = False, # default but here for clarity\n",
    "        )\n",
    "    # print(base_model_layer.get_config())\n",
    "    # print(base_model_layer.summary())\n",
    "    return base_model_layer\n",
    "\n",
    "def test(x):\n",
    "    tf.print(x.shape)\n",
    "    return x\n",
    "\n",
    "def build_model(\n",
    "    base_model_metadata,\n",
    "    num_classes,\n",
    "    dropout,\n",
    "):\n",
    "    model_handle, input_dimension, preprocess_input = base_model_metadata\n",
    "\n",
    "    model = Sequential([\n",
    "        # layers.Lambda(preprocess_input),\n",
    "        build_base_model_layer(\n",
    "            model_handle,\n",
    "        ),\n",
    "        layers.Dense(\n",
    "            num_classes,\n",
    "            # activation = 'softmax',\n",
    "        ),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Activation(\"softmax\", dtype=\"float32\"),\n",
    "    ])\n",
    "    \n",
    "    # model.build(\n",
    "    #     (batch_size,) + (input_dimension, input_dimension) + (3,),\n",
    "    # )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4286d70",
   "metadata": {},
   "source": [
    "## Build and run all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03b3f0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11788 files belonging to 200 classes.\n",
      "Using 9431 files for training.\n",
      "Using 2357 files for validation.\n",
      "WARNING:tensorflow:From /home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(None, 299, 299, 3)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(None,)\n",
      "\n",
      "keras.applications.Xception\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/keras/losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/keras/backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 200) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_id)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Save model\u001b[39;00m\n\u001b[1;32m     80\u001b[0m model\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(log_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_model\u001b[39m\u001b[38;5;124m'\u001b[39m ))    \n",
      "File \u001b[0;32m~/.conda/envs/fungi/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileblu15org.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/keras/losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/keras/backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 200) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "max_epochs = 5\n",
    "# dropout = 0.4\n",
    "dropout = 0.4\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Directory for logs\n",
    "base_log_dir = \"models_cub_logs\"\n",
    "\n",
    "# for each base model\n",
    "for base_model_metadata in base_models_metadata:\n",
    "    \n",
    "    model_handle, input_dimension, preprocess_input = base_model_metadata\n",
    "\n",
    "    image_size = (input_dimension, input_dimension)\n",
    "    \n",
    "    # Build dataset/pipeline\n",
    "    train_ds, val_ds, class_names = build_dataset(\n",
    "        batch_size = batch_size,\n",
    "        image_size = image_size,\n",
    "        preprocess_input = preprocess_input,\n",
    "    )\n",
    "    \n",
    "    # Build model\n",
    "    model = build_model(\n",
    "        base_model_metadata,\n",
    "        len(class_names),\n",
    "        dropout,\n",
    "    )\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        # loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(\n",
    "            # from_logits=True,\n",
    "        ),\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            # tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "            # tf.keras.metrics.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            # tf.keras.metrics.SparseTopKCategoricalAccuracy(k=3, name=\"Top3\"),\n",
    "            # tf.keras.metrics.SparseTopKCategoricalAccuracy(k=10, name=\"Top10\"),\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    # Logging\n",
    "    model_id = get_model_name(model_handle)\n",
    "    log_dir = os.path.join( base_log_dir, model_id )\n",
    "    \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=1,\n",
    "    )\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        # monitor='val_sparse_categorical_accuracy',\n",
    "        monitor='accuracy',\n",
    "        patience=5,\n",
    "        min_delta=0.001,\n",
    "    ),\n",
    "    \n",
    "    print()\n",
    "    print(model_id)\n",
    "    \n",
    "    # Train\n",
    "    model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=max_epochs,\n",
    "        callbacks=[\n",
    "            tensorboard_callback,\n",
    "            early_stopping_callback,\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    model.save(os.path.join(log_dir, 'final_model' ))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f61dc90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f37b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42dfc3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65505c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-fungi] *",
   "language": "python",
   "name": "conda-env-.conda-fungi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
