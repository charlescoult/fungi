{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518a0c58",
   "metadata": {},
   "source": [
    "# Models Exploration using CUB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b411778a",
   "metadata": {},
   "source": [
    "## References\n",
    "* [Transfer Learning with Hub](https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub)\n",
    "* [`tf.keras.utils.image_dataset_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory)\n",
    "* [Limiting GPU Memory Growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a285c8a1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2c0f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from keras.utils.layer_utils import count_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeaabc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_memory_growth(limit=True):\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, limit)\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5bbc7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "limit_memory_growth()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3e8d4f",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47fe9ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(\n",
    "    image_batch,\n",
    "    predicted_class_names,\n",
    "):\n",
    "    plt.figure(figsize=(10,9))\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    for n in range(30):\n",
    "        plt.subplot(6,5,n+1)\n",
    "        plt.imshow(image_batch[n])\n",
    "        plt.title(predicted_class_names[n])\n",
    "        plt.axis('off')\n",
    "    _ = plt.suptitle(\"Predictions\")\n",
    "\n",
    "def plot_images(\n",
    "    ds,\n",
    "    class_names,\n",
    "):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for images, labels in ds.tjake(1):\n",
    "        for i in range(9):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "            plt.title(class_names[labels[i]])\n",
    "            plt.axis(\"off\")\n",
    "    \n",
    "def get_timestamp():\n",
    "    return datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf8d406",
   "metadata": {},
   "source": [
    "## Enumerate Datasets to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b652b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "flowers_dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "flowers_data_dir = tf.keras.utils.get_file('flower_photos', origin=flowers_dataset_url, untar=True)\n",
    "flowers_data_dir = pathlib.Path(flowers_data_dir)\n",
    "\n",
    "datasets = [\n",
    "    '/mnt/cub/CUB_200_2011/images',\n",
    "    flowers_data_dir,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c9c943",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd46f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dataset object from a directory of images\n",
    "def build_dataset(\n",
    "    data_dir,\n",
    "    image_size,\n",
    "    preprocess_input = None,\n",
    "    batch_size = 64,\n",
    "):\n",
    "   \n",
    "    train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        batch_size = batch_size,\n",
    "        validation_split = 0.2,\n",
    "        image_size = image_size,\n",
    "        subset = \"both\",\n",
    "        shuffle = True, # default but here for clarity\n",
    "        seed=42,\n",
    "        label_mode=\"categorical\" # enables one-hot encoding (use 'int' for sparse_categorical_crossentropy loss)\n",
    "    )\n",
    "    \n",
    "    # Retrieve class names\n",
    "    # (can't do this after converting to PrefetchDataset?)\n",
    "    class_names = train_ds.class_names\n",
    "    \n",
    "    # Prefetch images\n",
    "    train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # apply preprocessing function\n",
    "    train_ds.map(\n",
    "        lambda x, y: (preprocess_input(x), y),\n",
    "    )\n",
    "    \n",
    "    return (train_ds, val_ds, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf56f3",
   "metadata": {},
   "source": [
    "## Enumerate Models to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee979c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of tuples describing the models to be tested\n",
    "# in the form: (model_handle, input_image_size, preprocessing_function)\n",
    "# where the model_handle is a model building function or a url to a tfhub feature model\n",
    "base_models_metadata = [\n",
    "    # ('https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4', 224),\n",
    "    # ('https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4', 299),\n",
    "    # ('https://tfhub.dev/google/inaturalist/inception_v3/feature_vector/5', 299),\n",
    "    (tf.keras.applications.Xception, 299, tf.keras.applications.xception.preprocess_input),\n",
    "    # (tf.keras.applications.resnet.ResNet101, 224),\n",
    "    # (tf.keras.applications.ResNet50, 224),\n",
    "    # (tf.keras.applications.InceptionResNetV2, 299),\n",
    "    # (tf.keras.applications.efficientnet_v2.EfficientNetV2B0, 224)\n",
    "]\n",
    "\n",
    "# return a name that accurately describes the model building function or\n",
    "# the tfhub model (by url) that was passed\n",
    "def get_model_name( model_handle ):\n",
    "    \n",
    "    if callable(model_handle):\n",
    "        return f'keras.applications.{model_handle.__name__}'\n",
    "    else:\n",
    "        split = model_handle.split('/')\n",
    "        return f'{split[-5]}.{split[-4]}.{split[-3]}'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aa7788",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a06cf38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model weight counts\n",
    "def print_weight_counts(model):\n",
    "    print(f'Full Model - Non-trainable weights: {count_params(model.non_trainable_weights)}')\n",
    "    print(f'Full Model - Trainable weights: {count_params(model.trainable_weights)}')\n",
    "    \n",
    "def build_base_model_layer(\n",
    "    model_handle,\n",
    "    name=\"base_model_layer\",\n",
    "):\n",
    "    # If model_handle is a model building function, use that function\n",
    "    if callable(model_handle):\n",
    "        base_model_layer = model_handle(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            pooling = 'avg',\n",
    "        )\n",
    "        # Freeze base model\n",
    "        base_model_layer.trainable = False\n",
    "    # otherwise build a layer from the tfhub url that was passed as a string\n",
    "    else:\n",
    "        base_model_layer = hub.KerasLayer(\n",
    "            model_handle,\n",
    "            name=name,\n",
    "            trainable = False, # default but here for clarity\n",
    "        )\n",
    "    \n",
    "    # Print Base model weights\n",
    "    print(\"Base Model:\")\n",
    "    print_weight_counts(base_model_layer)\n",
    "    print()\n",
    "    \n",
    "    return base_model_layer\n",
    "\n",
    "def build_model(\n",
    "    base_model_metadata,\n",
    "    num_classes,\n",
    "    dropout,\n",
    "):\n",
    "    # Get base_model_information\n",
    "    model_handle, input_dimension, preprocess_input = base_model_metadata\n",
    "\n",
    "    # Build Full model\n",
    "    model = Sequential([\n",
    "        \n",
    "        build_base_model_layer(\n",
    "            model_handle,\n",
    "        ),\n",
    "        \n",
    "        layers.Dense(\n",
    "            num_classes,\n",
    "            # activation = 'softmax',\n",
    "        ),\n",
    "        \n",
    "        layers.Dropout(dropout),\n",
    "        layers.Activation(\"softmax\", dtype=\"float32\"),\n",
    "\n",
    "    ])\n",
    "    \n",
    "    # Print weight counts\n",
    "    print(\"Full Model:\")\n",
    "    print_weight_counts(model)\n",
    "    print()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4286d70",
   "metadata": {},
   "source": [
    "## Build and run all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b63d8a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for logs\n",
    "base_log_dir = f'models_cub_02_logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03b3f0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11788 files belonging to 200 classes.\n",
      "Using 9431 files for training.\n",
      "Using 2357 files for validation.\n",
      "WARNING:tensorflow:From /home/charlescoult/.conda/envs/fungi/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "Base Model:\n",
      "Full Model - Non-trainable weights: 20861480\n",
      "Full Model - Trainable weights: 0\n",
      "\n",
      "Full Model:\n",
      "Full Model - Non-trainable weights: 20861480\n",
      "Full Model - Trainable weights: 409800\n",
      "\n",
      "Epoch 1/5\n",
      "148/148 [==============================] - 33s 195ms/step - loss: 39.6085 - accuracy: 0.0061 - val_loss: 16.5337 - val_accuracy: 0.0072\n",
      "Epoch 2/5\n",
      "148/148 [==============================] - 24s 160ms/step - loss: 17.9117 - accuracy: 0.0066 - val_loss: 13.9473 - val_accuracy: 0.0064\n",
      "Epoch 3/5\n",
      "148/148 [==============================] - 24s 162ms/step - loss: 14.9294 - accuracy: 0.0104 - val_loss: 12.6120 - val_accuracy: 0.0068\n",
      "Epoch 4/5\n",
      "148/148 [==============================] - 24s 160ms/step - loss: 13.3279 - accuracy: 0.0098 - val_loss: 11.5892 - val_accuracy: 0.0089\n",
      "Epoch 5/5\n",
      "148/148 [==============================] - 24s 163ms/step - loss: 12.2932 - accuracy: 0.0129 - val_loss: 10.9380 - val_accuracy: 0.0089\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "max_epochs = 5\n",
    "dropout = 0.4\n",
    "learning_rate = 0.0001\n",
    "\n",
    "\n",
    "# for each base model\n",
    "for base_model_metadata in base_models_metadata:\n",
    "    \n",
    "    model_handle, input_dimension, preprocess_input = base_model_metadata\n",
    "\n",
    "    image_size = (input_dimension, input_dimension)\n",
    "    \n",
    "    # Build dataset/pipeline\n",
    "    train_ds, val_ds, class_names = build_dataset(\n",
    "        datasets[0],\n",
    "        batch_size = batch_size,\n",
    "        image_size = image_size,\n",
    "        preprocess_input = preprocess_input,\n",
    "    )\n",
    "    \n",
    "    # Build model\n",
    "    model = build_model(\n",
    "        base_model_metadata,\n",
    "        len(class_names),\n",
    "        dropout,\n",
    "    )\n",
    "    \n",
    "    # Compile model\n",
    "    # Sparse vs non-sparse CCE https://www.kaggle.com/general/197993\n",
    "    model.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        # loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(\n",
    "            # from_logits=True,\n",
    "        ),\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            # tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "            # tf.keras.metrics.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            # tf.keras.metrics.SparseTopKCategoricalAccuracy(k=3, name=\"Top3\"),\n",
    "            # tf.keras.metrics.SparseTopKCategoricalAccuracy(k=10, name=\"Top10\"),\n",
    "        ],\n",
    "    )\n",
    "            \n",
    "    # Logging\n",
    "    model_id = get_model_name(model_handle)\n",
    "    log_dir = os.path.join( base_log_dir, model_id )\n",
    "    \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=1,\n",
    "    )\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        # monitor='val_sparse_categorical_accuracy',\n",
    "        monitor='accuracy',\n",
    "        patience=5,\n",
    "        min_delta=0.001,\n",
    "    ),\n",
    "    \n",
    "    # Train\n",
    "    model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=max_epochs,\n",
    "        callbacks=[\n",
    "            tensorboard_callback,\n",
    "            early_stopping_callback,\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # \n",
    "    \n",
    "    # Save model\n",
    "    # model.save(os.path.join(log_dir, 'final_model' ))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f61dc90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f37b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42dfc3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65505c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-fungi] *",
   "language": "python",
   "name": "conda-env-.conda-fungi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
